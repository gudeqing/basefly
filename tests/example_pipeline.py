import os
script_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
import sys; sys.path.append(script_path)
from basefly.basefly import Argument, Output, Command, Workflow, TopVar, ToWdlTask
from utils.get_fastq_info import get_fastq_info
__author__ = 'gdq'

"""
提示:
0. 写workflow时，参数赋值规范建议：args[X].value = TopVar[?] | task.Outputs[?] | TmpVar()
*. 如果不是为了写wdl流程，可以不使用TmpVar，直接赋值就ok
1. 一定要正确定义参数的类型, type is one of ['str', 'int', 'float', 'bool', 'infile', 'indir', 'fix']
    其中‘fix'可以用于表示命令行中的固定字符串或固定参数, 如 “bwa xxx | samtools sort -" 中的‘| samtools sort -’ 可以用fix固定
2. 参数的添加顺序对于命令行的正确形成很重要，这里字典的有序性得到利用
3. 定义output时，value(或path）属性对应的值可以直接用{}引用cmd.args的key，

关于runtime:
memory和cpu是定义最小计算资源需求
max_memory和max_cpu定义计算资源上限
image: 定义docker镜像
tool：工具命令，即命令行的第一个参数
tool_dir: 定义tool所在路径

关于meta：
name: 定义命令行的名称，会参与具体task的name的形成，建议组成：[数字，字母，’-‘], 下划线会自动被替换为中划线’-‘
其他字段都是描述工具的开发作者(author)，链接(source)，版本号(version)，简介（desc)
"""


def fastp():
    cmd = Command()
    # 定义命令行的名称, 说明信息，完善这些有利于将来自动生成流程的说明文档
    cmd.meta.name = 'fastp'
    cmd.meta.desc = 'this a tool that can process fastq effectively'
    # 定义镜像地址
    cmd.runtime.image = 'gudeqing/fastp:0.23.2'
    # 定义工具路径
    cmd.runtime.tool = 'fastp'
    # 定义该工具需要的最少内存, 单位是bite,下面设置为1G
    cmd.runtime.memory = 1024**3
    # 定义该工具做多可以使用多少存储，下面设为10G
    cmd.runtime.max_memory = 10*1024**3
    # 使用字典语法添加参数对象
    cmd.args['read1'] = Argument(prefix='-i ', type='infile', desc='read1 fastq file')
    cmd.args['read2'] = Argument(prefix='-I ', type='infile', desc='read2 fastq file')
    cmd.args['out1'] = Argument(prefix='-o ', type='str', desc='clean read1 output fastq file')
    cmd.args['out2'] = Argument(prefix='-O ', type='str', desc='clean read2 output fastq file')
    # 使用字典语法添加output对象, 这里允许使用”{}“引用其他Argument对象作为输入
    cmd.outputs['out1'] = Output(value="{out1}", type='outfile')
    cmd.outputs['out2'] = Output(value="{out2}")
    return cmd


def salmon():
    # 定义一个command
    cmd = Command()
    cmd.meta.name = 'salmon'
    cmd.meta.desc = 'transcript expression quantification'
    cmd.runtime.image = "combinelab/salmon:latest"
    cmd.runtime.memory = 2*1024**3
    cmd.runtime.cpu = 2
    cmd.runtime.tool = 'salmon quant'
    cmd.args['libType'] = Argument(prefix='--libType ', default='A', desc='默认自动判定文库类型')
    cmd.args['indexDir'] = Argument(prefix='-i ', type='indir', desc='transcript fasta index directory')
    cmd.args['read1'] = Argument(prefix='-1 ', type='infile', desc='read1 fastq file')
    cmd.args['read2'] = Argument(prefix='-2 ', type='infile', desc='read2 fastq file')
    cmd.args['outDir'] = Argument(prefix='-o ', type='str', default='quant', desc='output directory')
    cmd.args['gcBias'] = Argument(prefix='--gcBias ', type='bool', default=True, desc='perform gc Bias correction')
    cmd.outputs['transcript'] = Output(path="{outDir}" + "/quant.sf")
    cmd.outputs['outDir'] = Output(path="{outDir}", type='outdir')
    return cmd


def quant_merge():
    cmd = Command()
    cmd.meta.name = 'quantMerge'
    cmd.meta.desc = 'Merge multiple quantification results into a single file'
    cmd.runtime.image = "combinelab/salmon:latest"
    cmd.runtime.tool = 'salmon quantmerge'
    # 下面的quants参数对应的是目录，可以接收多个值作为输入
    cmd.args['quants'] = Argument(prefix="--quants ", array=True, type='indir', desc='salmon quant dir list')
    cmd.args['names'] = Argument(prefix='--names ', array=True, level='optional')
    cmd.args['column'] = Argument(prefix='--column ', default='TPM')
    cmd.args['genes'] = Argument(prefix='--genes ', type='bool', default=False)
    cmd.args['out'] = Argument(prefix='--output ', default=f'merged.{cmd.args["column"].default}.txt')
    cmd.outputs['result'] = Output(path="{out}")
    return cmd


def pipeline():
    """
    从这里开始编排分析流程
    """
    wf = Workflow()
    wf.meta.name = 'fast gene quantification using fastp and salmon'
    wf.meta.source = "you may provide a link here"
    wf.meta.desc = """
        This is a pipeline that can do gene quantification quickly
        you are required to inputs:
        1. fastq information
        2. transcriptome index directory generated by salmon
    """
    wf.meta.version = "0.1.1"

    # 定义流程输入参数
    # 初始化流程输入参数解析器
    wf.init_argparser()
    wf.add_argument('-fastq_info', nargs='+', required=True,
                    help='A list with elements from [fastq file, fastq parent dir, fastq_info.txt, fastq_info.json]')
    wf.add_argument('-r1_name', default='(.*).R1.fastq',
                    help="python regExp that describes the full name of read1 fastq file name. "
                         "It requires at least one pair small brackets, "
                         "and the string matched in the first pair brackets will be used as sample name. "
                         "Example: '(.*).R1.fq.gz'")
    wf.add_argument('-r2_name', default='(.*).R2.fastq',
                    help="python regExp that describes the full name of read2 fastq file name. "
                         "It requires at least one pair small brackets,"
                         " and the string matched in the first pair brackets will be used as sample name. "
                         "Example: '(.*).R2.fq.gz'")
    wf.add_argument('-exclude_samples', default=tuple(), nargs='+', help='specify samples to be excluded from analysis')
    wf.add_argument('-indexDir', help='The path to the folder containing the salmon indexing files of transcriptome')
    # 开始收集参数
    wf.parse_args()

    # 进行具体的流程编排任务
    # 提取fastq信息
    fastq_info = get_fastq_info(fastq_info=wf.args.fastq_info, r1_name=wf.args.r1_name, r2_name=wf.args.r2_name)
    if len(fastq_info) <= 0:
        raise Exception('No fastq file found !')
    # 考虑将流程的输入文件信息放入到TopVar, 可以方便将来把流程转换为其他格式的流程，如Argo Workflow，当然，你也可以选择不这么做
    top_vars = dict(
        index_dir=TopVar(value=wf.args.indexDir, type='indir')
    )
    wf.topvars = top_vars

    merge_depends = []
    for sample, (r1, r2) in fastq_info.items():
        # 向流程中添加task
        fastp_task, args = wf.add_task(fastp(), name='fastp-'+sample)
        args['read1'].value = r1[0]  # 如果read1有多个文件，也仅仅使用第一个作为输入，暂不设计处理多个的情况
        args['read2'].value = r2[0]
        args['out1'].value = f'{sample}.clean.R1.fq'
        args['out2'].value = f'{sample}.clean.R2.fq'

        # 添加第二个task
        task, args = wf.add_task(salmon(), name='salmon-'+sample)
        task.depends = [fastp_task.task_id]
        args['read1'].value = fastp_task.outputs["out1"]
        args['read2'].value = fastp_task.outputs["out2"]
        args['indexDir'].value = wf.topvars['index_dir']
        args['outDir'].value = sample

        # 收集多个目标task，作为其他的输入
        merge_depends.append(task.task_id)

    # merge transcript TPM
    task, args = wf.add_task(quant_merge())
    # 由于同一个command被使用多次，这里要记得重新命名
    task.cmd.meta.name = 'MergeTranscriptTPM'
    task.depends = merge_depends
    # 针对当前任务，column 和 genes 参数需要固定，不允许修改,用 'fix' 表示
    args['column'].value = 'TPM'
    args['column'].type = 'fix'
    args['genes'].value = False
    args['genes'].type = 'fix'
    args['quants'].value = [wf.tasks[task_id].outputs['outDir'] for task_id in task.depends]

    # merge transcript Count
    task, args = wf.add_task(quant_merge())
    task.cmd.meta.name = 'MergeTranscriptCount'
    task.depends = merge_depends
    # 针对当前任务，column 和 genes 参数需要固定，不允许修改
    args['column'].value = 'NumReads'
    args['column'].type = 'fix'
    args['genes'].value = False
    args['genes'].type = 'fix'
    args['quants'].value = [wf.tasks[task_id].outputs['outDir'] for task_id in task.depends]
    # 由于生成wdl的task时，优先使用参数的默认值，所以下面直接修改默认值
    args['out'].default = f'merged.{args["column"].value}.txt'

    # 流程编排结束，运行流程
    wf.run()

    if wf.success:
        print('you can do something such as clean your workspace here')

    # 你也可以选择生成其他版本的流程
    wf.to_argo_worflow(f'{wf.meta.name}.yaml')
    wf.to_wdl_tasks(f'{wf.meta.name}.tasks.wdl')


if __name__ == '__main__':
    pipeline()
